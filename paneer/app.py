import os
import sys

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_mistralai import ChatMistralAI
from langchain_core.tools import Tool
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage

from dotenv import load_dotenv

load_dotenv()

DB_DIRECTORY = "nitt_vector_db"
EMBEDDING_MODEL = "all-MiniLM-L6-v2"
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")

def format_docs(docs):
    """Helper to join retrieved document chunks into a single string."""
    return "\n\n".join(doc.page_content for doc in docs)

def start_wikinitt_chat():
    if not MISTRAL_API_KEY:
        print("Error: MISTRAL_API_KEY not found. Please set it.")
        return

    if not os.path.exists(DB_DIRECTORY):
        print(f"Error: DB directory '{DB_DIRECTORY}' not found.")
        return

    print("Loading Embedding Model...")
    embedding_function = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

    print(f"Loading Database...")
    vector_db = Chroma(
        persist_directory=DB_DIRECTORY, 
        embedding_function=embedding_function
    )
    retriever = vector_db.as_retriever(search_kwargs={"k": 5})
    
    def search_nitt_func(query: str):
        """Searches for information about NIT Trichy."""
        print(f"   (üîç Searching: {query})")
        docs = retriever.invoke(query)
        return format_docs(docs)

    tool = Tool(
        name="search_nitt_data",
        func=search_nitt_func,
        description="Searches for information about NIT Trichy, courses, events, campus details, and academic regulations. Use this whenever you need factual information about the institute."
    )
    tools = [tool]
    tools_map = {t.name: t for t in tools}

    llm = ChatMistralAI(
        model="mistral-small-latest",
        temperature=0.3,
        mistral_api_key=MISTRAL_API_KEY
    )
    
    llm_with_tools = llm.bind_tools(tools)

    system_message = SystemMessage(content="""You are WikiNITT, an intelligent and deep-thinking AI assistant for NIT Trichy.

    CORE RESPONSIBILITIES:
    1. **Deep Reasoning**: Do not just answer immediately. Think about what information you need.
    2. **Multi-Step Search**: If the user's query is complex, break it down. You can search multiple times if the first search doesn't give you everything.
    3. **Context Awareness**: Remember previous interactions.
    4. **Honesty & Clarity**: If you cannot find the answer in the context or tools, admit it. Do not hallucinate.

    PROCESS:
    - Step 1: Analyze the user's request.
    - Step 2: Formulate a plan. Ask yourself "What do I need to know?".
    - Step 3: Use the `search_nitt_data` tool to gather facts.
    - Step 4: Analyze the search results. Is it enough? If not, search again with a better query.
    - Step 5: Synthesize the final answer.
    """)

    print("\n" + "="*50)
    print("ü§ñ WikiNITT is Ready! (Deep Agentic Mode - Mistral)")
    print("="*50 + "\n")

    chat_history = [] 

    while True:
        try:
            user_input = input("You: ")
            if user_input.lower() in ["exit", "quit", "q"]:
                break
            
            messages = [system_message] + chat_history + [HumanMessage(content=user_input)]
            
            print("   (üß† Thinking...)")
            
            # Dynamic Agent Loop
            while True:
                response = llm_with_tools.invoke(messages)
                messages.append(response)
                
                if not response.tool_calls:
                    final_answer = response.content
                    
                    if not final_answer or (isinstance(final_answer, str) and not final_answer.strip()):
                        print("   (‚ö†Ô∏è Model returned empty response. Retrying with a nudge...)")
                        messages.append(HumanMessage(content="Please provide your answer or use a tool."))
                        continue

                    print(f"\nWikiNITT: {final_answer}\n")
                    
                    chat_history.append(HumanMessage(content=user_input))
                    chat_history.append(AIMessage(content=str(final_answer)))
                    break
                
                if len(messages) > 20: 
                    print("   (‚ö†Ô∏è Max recursion depth reached. Stopping.)")
                    break

                for tool_call in response.tool_calls:
                    tool_name = tool_call["name"]
                    tool_args = tool_call["args"]
                    
                    if tool_name in tools_map:
                        print(f"   (üõ†Ô∏è  Tool Call: {tool_name})") 
                        
                        try:
                            tool_result = tools_map[tool_name].invoke(tool_args)
                        except Exception as tool_err:
                            tool_result = f"Error executing tool: {tool_err}"
                        
                        messages.append(ToolMessage(
                            tool_call_id=tool_call["id"],
                            content=str(tool_result)
                        ))
                    else:
                        print(f"   (‚ö†Ô∏è Unknown Tool: {tool_name})")
                        messages.append(ToolMessage(
                            tool_call_id=tool_call["id"],
                            content=f"Error: Tool '{tool_name}' not found. Please use a valid tool."
                        ))

        except Exception as e:
            print(f"‚ö†Ô∏è Error: {e}")

if __name__ == "__main__":
    start_wikinitt_chat()